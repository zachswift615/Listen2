# Session Handoff: Word Highlighting State Contamination Fix

**Date:** 2025-11-16
**Time:** 16:43:54
**Session Duration:** ~3 hours
**Status:** ⚠️ Partial - Fixed state contamination but introduced phoneme duration corruption

---

## Executive Summary

This session focused on fixing the root cause of word highlighting issues in the Listen2 TTS app. We successfully identified and fixed a **race condition in shared mutable state** that was causing normalized text contamination. However, the fix introduced a new issue: **phoneme duration corruption** that breaks word timing calculations.

**Current State:**
- ✅ Fixed: State contamination (normalized text from wrong sentences)
- ✅ Fixed: espeak-ng buffer contamination (added `espeak_Cancel()`)
- ❌ New Issue: Phoneme durations are corrupted (huge/negative values)
- ❌ Symptom: Only first word of each sentence highlights correctly

---

## Problem Analysis from Logs

### Original Issue (Before This Session)

**Symptoms:**
- Wrong normalized text appearing (e.g., "for each message..." instead of "this repetitive...")
- Phoneme count mismatches (185 phonemes for a 17-word sentence)
- Words being skipped during highlighting
- "Cache hit" returning stale data

**Log Evidence (line 3485-3560):**
```
[DEBUG] About to phonemize text: 'This repetitive two-minute process...'
[PIPER-PHONEMIZE] Processing 17 words with positions
[DEBUG] New normalized text is: 'this repetitive two minute process...'
BUT THEN:
[SherpaOnnx] C API returned: num_phonemes=185 (way too many!)
[SherpaOnnx] Extracted normalized text: 'for each message a human agent...' ❌ WRONG!
```

**Root Cause Identified:**
Race condition in `PiperPhonemizeLexicon` shared mutable state:
1. Thread A calls `ConvertTextToTokenIds("This repetitive...")`
2. Updates shared `last_normalized_text_` = "this repetitive..."
3. Thread A starts returning
4. **Thread B (lookahead synthesis) calls `ConvertTextToTokenIds("For each message...")`**
5. Overwrites shared `last_normalized_text_` = "for each message..."
6. Thread A tries to read `last_normalized_text_` → gets wrong data!

---

## Work Completed

### 1. Added `espeak_Cancel()` to piper-phonemize ✅

**File:** `/Users/zachswift/projects/piper-phonemize/src/phonemize.cpp:260`

**Change:**
```cpp
// Set up synthesis callback to capture events
espeak_SetSynthCallback(synth_callback);

// CRITICAL: Cancel any pending espeak synthesis from previous calls
// This clears espeak's internal text buffer
espeak_Cancel();

// CRITICAL: Clear thread_local phoneme capture state
g_phoneme_capture.phoneme_positions.clear();
```

**Why:** espeak-ng buffers text from previous calls, causing extra phoneme groups.

**Result:** ✅ Eliminates espeak-level buffer contamination

---

### 2. Fixed State Contamination in sherpa-onnx ✅

**Problem:** Shared mutable state in `PiperPhonemizeLexicon` getting overwritten by concurrent calls.

**Solution:** Store phoneme data directly in `TokenIDs` struct to return it atomically.

#### Changes Made:

**A. Extended TokenIDs struct**
**File:** `sherpa-onnx/csrc/offline-tts-frontend.h`

```cpp
struct TokenIDs {
  std::vector<int64_t> tokens;
  std::vector<int64_t> tones;  // existing

  // NEW: Phoneme data (eliminates race condition)
  std::vector<PhonemeSequence> phoneme_sequences;
  std::string normalized_text;
  std::vector<std::pair<int32_t, int32_t>> char_mapping;
};
```

**B. Populate TokenIDs in PiperPhonemizeLexicon**
**File:** `sherpa-onnx/csrc/piper-phonemize-lexicon.cc`

Updated both:
- `ConvertTextToTokenIdsMatcha()` (line 628-636)
- `ConvertTextToTokenIdsVits()` (line 717-725)

```cpp
// Store phoneme data atomically with tokens
std::vector<PhonemeSequence> sequences = {phoneme_info};
for (auto& token_id : ans) {
  token_id.phoneme_sequences = sequences;
  token_id.normalized_text = last_normalized_text_;
  token_id.char_mapping = last_char_mapping_;
}
```

**C. Extract from TokenIDs**
**File:** `sherpa-onnx/csrc/offline-tts-vits-impl.h:218-237`

```cpp
// NEW: Extract phoneme data directly from TokenIDs (no race!)
std::vector<PhonemeSequence> phoneme_sequences;
std::string normalized_text;
std::vector<std::pair<int32_t, int32_t>> char_mapping;

if (!token_ids.empty() && !token_ids[0].phoneme_sequences.empty()) {
  // Flatten all phoneme sequences
  for (const auto& token_id : token_ids) {
    for (const auto& seq : token_id.phoneme_sequences) {
      phoneme_sequences.push_back(seq);
    }
  }
  normalized_text = token_ids[0].normalized_text;
  char_mapping = token_ids[0].char_mapping;
}
```

**Result:** ✅ Eliminates race condition - each call gets its own data atomically

---

### 3. Framework Rebuild & Update ✅

**Commands executed:**
```bash
cd /Users/zachswift/projects/Listen2
./scripts/update-frameworks.sh --build
```

**Rebuilt:**
- sherpa-onnx.xcframework (all architectures)
- VoxPDFCore.xcframework

**Updated:**
- Copied to `Frameworks/` directory in Listen2 project
- Build timestamp: 2025-11-16 15:16

---

## NEW ISSUE DISCOVERED ❌

### Symptom: Phoneme Duration Corruption

**Observed behavior:**
- Only the **first word** of each sentence highlights
- When next sentence starts, its first word highlights
- All other words never highlight

**Log Evidence (after fix):**

```
Line 337: [SherpaOnnx] Extracted 7 phonemes (durations: ✓, total: -14486.973s) ❌
Line 382: [SherpaOnnx] Extracted 12 phonemes (durations: ✓, total: -66314.394s) ❌
Line 400: Word[0] 'CHAPTER' @ 0.000s for -122716.405s ❌ HUGE NEGATIVE!
Line 401: Word[1] '2' @ -122716.405s for 15538.438s ❌ HUGE POSITIVE!
```

**Expected values:**
```
Word[0] 'CHAPTER' @ 0.000s for 0.186s ✓
Word[1] '2' @ 0.186s for 0.104s ✓
```

**Magnitude of error:** Values are **1,000,000x too large** and often negative!

---

## Root Cause Hypothesis: Duration Data Corruption

### ⚠️ CRITICAL CLUE FROM PREVIOUS SESSION (Nov 14, 2025)

**This EXACT symptom was fixed 2 days ago!** See: `docs/HANDOFF_2025-11-14_WCEIL_DTYPE_FIX.md`

**Previous issue:** float32 → int64 dtype mismatch when reading w_ceil tensor
- Model exported w_ceil as float32
- sherpa-onnx C++ read it as int64
- Result: Garbage values like -2,147,483,648 or 1,073,741,824

**Fix applied (Nov 14):**
- File: `piper/src/python/piper_train/export_onnx.py:69`
- Added `.squeeze().long()` to cast w_ceil to int64
- Commit: `0dad73a`

**BUT** - Our new TokenIDs code flow may have broken the w_ceil extraction path!

### Two Possible Causes

**1. Model regression** - Model needs to be re-exported with the `.long()` fix

**2. Our TokenIDs changes broke w_ceil extraction** - The new code path in `offline-tts-vits-impl.h` may be:
- Not calling the TTS model correctly
- Not extracting w_ceil tensor
- Not populating `phoneme_durations` in `GeneratedAudio`

### Likely Issue: Phoneme Duration Not Being Copied

The phoneme duration data is **not** in `PhonemeSequence` (which only has `PhonemeInfo` with symbols and positions). The durations come from a **separate** `phoneme_durations` vector in `GeneratedAudio`.

**The problem:**
1. We're storing `phoneme_sequences` in `TokenIDs` ✓
2. BUT we're **NOT** storing `phoneme_durations` ❌
3. When the TTS model generates audio, it produces durations from the `w_ceil` tensor
4. These durations are stored in a **different** place than `PhonemeSequence`
5. Our new code path may be losing this connection

**Where durations come from:**
- File: `sherpa-onnx/csrc/offline-tts-vits-impl.h:590-595`
- Extracted from ONNX model's `w_ceil` tensor output
- Stored in `GeneratedAudio.phoneme_durations`

**The disconnect:**
```cpp
// We store this in TokenIDs:
phoneme_sequences (symbols + positions)

// But NOT this:
phoneme_durations (samples from w_ceil tensor)

// Later, alignment tries to use durations but they're corrupted!
```

---

## Next Steps & Recommendations

### Immediate Fix Required

**Option 1: Store durations in TokenIDs (Recommended)**

1. Add `phoneme_durations` field to `TokenIDs` struct
2. Capture durations from TTS model output
3. Store in TokenIDs alongside phoneme_sequences
4. Extract and use in alignment

**Option 2: Verify duration extraction is still working**

The corruption might be from flattening logic gone wrong. Check:
- `offline-tts-vits-impl.h:590-595` - w_ceil extraction
- `c-api.cc:1325-1330` - duration copying to C struct
- `SherpaOnnx.swift:150-280` - Swift extraction

### IMMEDIATE DIAGNOSTIC

**Run this first to check if model has correct dtype:**

```bash
cd ~/projects/piper
source venv/bin/activate

python3 << 'EOF'
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession(
    "/Users/zachswift/projects/Listen2/Listen2/Listen2/Listen2/Resources/PiperModels/en_US-lessac-high.onnx"
)

dummy_phonemes = np.array([[1, 2, 3, 4, 5]], dtype=np.int64)
dummy_lengths = np.array([5], dtype=np.int64)
dummy_scales = np.array([0.667, 1.0, 0.8], dtype=np.float32)

outputs = session.run(None, {
    "input": dummy_phonemes,
    "input_lengths": dummy_lengths,
    "scales": dummy_scales,
})

print(f"w_ceil dtype: {outputs[1].dtype}")
print(f"w_ceil shape: {outputs[1].shape}")
print(f"w_ceil values: {outputs[1]}")
print(f"After ×256: {outputs[1] * 256}")
EOF
```

**Expected output:**
```
w_ceil dtype: int64  ✅
w_ceil shape: (5,)   ✅
w_ceil values: [2 1 2 1 2]
After ×256: [512 256 512 256 512]
```

**If you see float32 or wrong shape:** Re-export model with `./scripts/export-and-update-model.sh`

### Investigation Steps

1. **Add debug logging** to track duration values at each stage:
   ```cpp
   fprintf(stderr, "[DEBUG] w_ceil[0] = %ld\n", w_ceil_values[0]);
   fprintf(stderr, "[DEBUG] phoneme_duration[0] = %d\n", phoneme_durations[0]);
   ```

2. **Check if durations are in TokenIDs correctly:**
   - Does `TokenIDs` need a `phoneme_durations` field?
   - Are durations being lost during the new TokenIDs flow?

3. **Verify alignment calculation:**
   - `PhonemeAlignmentService.swift:150-280`
   - Is it using the corrupted durations?
   - Where does it get duration from `PhonemeInfo`?

### Testing Checklist

After fix:
- [ ] Verify "CHAPTER 2" shows reasonable durations (< 1 second total)
- [ ] Verify all words highlight in sequence
- [ ] Verify no negative durations
- [ ] Verify normalized text matches original text
- [ ] Test with multi-sentence paragraphs

---

## Architecture Notes

### Data Flow (Current State)

```
piper-phonemize
  ↓ (phonemes + positions)
sherpa-onnx C++ (ConvertTextToTokenIds)
  ↓ stores in TokenIDs ✓
  ↓ (tokens + phoneme_sequences + normalized_text)
TTS Model Inference
  ↓ generates w_ceil tensor
  ↓ extracts durations ← WHERE ARE THESE GOING?
GeneratedAudio
  ↓ (phonemes + durations + normalized_text)
C API
  ↓
Swift (SherpaOnnx.swift)
  ↓
Alignment Service ← GETS CORRUPTED DURATIONS
```

**The question:** Where in this flow are durations being corrupted?

---

## Files Modified

### piper-phonemize
- `/Users/zachswift/projects/piper-phonemize/src/phonemize.cpp:260` - Added `espeak_Cancel()`

### sherpa-onnx
- `/Users/zachswift/projects/sherpa-onnx/sherpa-onnx/csrc/offline-tts-frontend.h:17-43` - Extended TokenIDs struct
- `/Users/zachswift/projects/sherpa-onnx/sherpa-onnx/csrc/piper-phonemize-lexicon.cc:628-636, 717-725` - Populate TokenIDs
- `/Users/zachswift/projects/sherpa-onnx/sherpa-onnx/csrc/offline-tts-vits-impl.h:218-237` - Extract from TokenIDs

### Frameworks Updated
- `sherpa-onnx.xcframework` - Rebuilt with fixes
- `VoxPDFCore.xcframework` - Updated

---

## Key Insights

1. **State contamination was real** - logs clearly showed wrong normalized text from previous calls
2. **Atomic return pattern works** - storing data in TokenIDs eliminates race condition
3. **But durations are separate** - they come from TTS model output, not from phonemization
4. **The fix broke durations** - somehow the new code path loses or corrupts duration data

---

## Resources

**Log Files:**
- `/Users/zachswift/listen-2-logs-2025-11-13.txt` (2175 lines) - Latest logs showing duration corruption

**Architecture Documentation:**
- `/Users/zachswift/projects/Listen2/docs/WORD_HIGHLIGHTING_ARCHITECTURE.md` - Complete system overview
- Sections 488-600: Duration extraction from w_ceil tensor

**Workshop Context:**
```bash
workshop why "phoneme duration"
workshop context
```

---

## Questions for Next Session

1. **Where are phoneme durations supposed to be stored in the new TokenIDs flow?**
   - Should TokenIDs have a `phoneme_durations` field?
   - Or should durations stay separate from TokenIDs?

2. **Why are durations showing huge/negative values?**
   - Uninitialized memory?
   - Wrong pointer arithmetic during flattening?
   - Type mismatch (int32 vs int64)?

3. **Is the duration extraction from w_ceil still working?**
   - Check `offline-tts-vits-impl.h:590-595`
   - Verify values before corruption happens

4. **Should we revert and try a different approach?**
   - Maybe use mutex/lock on mutable state instead?
   - Or copy data more carefully in current approach?

---

## Success Criteria (Not Yet Met)

- [ ] Normalized text matches original (ACHIEVED ✓)
- [ ] Phoneme count matches word count (UNKNOWN - may be achieved)
- [ ] Phoneme durations in reasonable range (0.01s - 0.5s) (FAILED ❌)
- [ ] All words highlight in sequence (FAILED ❌)
- [ ] No negative durations (FAILED ❌)
- [ ] Total duration < 10s for typical sentence (FAILED ❌)

---

**Next developer: Start by investigating phoneme duration extraction in the new TokenIDs flow. The state contamination fix is solid, but we're losing/corrupting duration data somewhere.**
